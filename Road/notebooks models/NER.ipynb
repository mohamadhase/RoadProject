{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "path = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "if path not in sys.path:\n",
    "    sys.path.append(path)\n",
    "from constants import NER\n",
    "# get the first key in NERa\n",
    "from fuzzywuzzy import fuzz\n",
    "from constants import NER\n",
    "from fuzzywuzzy import fuzz\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import sklearn_crfsuite\n",
    "import sklearn_crfsuite\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from transformers import pipeline\n",
    "from utils.helpers import combine_sub_words\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/data_for_ner.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word(word, NER):\n",
    "    for key in NER.keys():\n",
    "        if any(fuzz.ratio(word, k) >= 80 for k in key):\n",
    "            return NER[key]\n",
    "    return \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ner_string(text, NER):\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    words = text.split()\n",
    "    results = Parallel(n_jobs=num_cores)(\n",
    "        delayed(process_word)(word, NER) for word in words)\n",
    "    return \" \".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ner'] = Parallel(n_jobs=-1)(\n",
    "    delayed(generate_ner_string)(text, NER) for text in df['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"CAMeL-Lab/bert-base-arabic-camelbert-ca-pos-egy\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) # how to make the tokinzation on split by space?\n",
    "\n",
    "pos_pipeline = pipeline(\"ner\", model=model_name, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nasser\\.conda\\envs\\finall\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n",
      "c:\\Users\\nasser\\.conda\\envs\\finall\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# remove any punctuation from the full text\n",
    "df['full_text'] = df['full_text'].str.replace(r'[^\\u0600-\\u06FF\\s]+', '')\n",
    "df['full_text'] = df['full_text'].str.replace(r'\\?|؟|،', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "pos_list = []\n",
    "for text in df['full_text']:\n",
    "    # remove any punctuation from the text\n",
    "    res = pos_pipeline(text)\n",
    "    res = combine_sub_words(res)\n",
    "    while(1):\n",
    "        if any('##' in r['word'] for r in res):\n",
    "            res = combine_sub_words(res)\n",
    "        else:\n",
    "            break\n",
    "    res = [r['entity'] for r in res]\n",
    "    res = \" \".join(res)\n",
    "    pos_list.append(res)\n",
    "df['pos'] = pos_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['full_text_len'] = df['full_text'].apply(lambda x: len(x.split()))\n",
    "df['pos_len'] = df['pos'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df[['full_text','pos', 'full_text_len', 'pos_len']]\n",
    "# print the rows that have different length\n",
    "df[df['full_text_len'] != df['pos_len']]\n",
    "# remove the rows that have different length\n",
    "df = df[df['full_text_len'] == df['pos_len']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9766, 5)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['full_text', 'pos', 'ner']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ner'] = data['ner'].astype(str)\n",
    "data['ner'] = data['ner'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation datasets\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "# Tokenize the input sequence\n",
    "input_tokenizer = Tokenizer( oov_token='UNK')\n",
    "input_tokenizer.fit_on_texts(train_data['full_text'])\n",
    "train_input_seq = input_tokenizer.texts_to_sequences(train_data['full_text'])\n",
    "val_input_seq = input_tokenizer.texts_to_sequences(val_data['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the output sequence\n",
    "output_tokenizer = Tokenizer(filters='', lower=False)\n",
    "output_tokenizer.fit_on_texts(train_data['ner'])\n",
    "train_output_seq = output_tokenizer.texts_to_sequences(train_data['ner'])\n",
    "val_output_seq = output_tokenizer.texts_to_sequences(val_data['ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word to index and index to word mappings for the input sequence\n",
    "input_word2idx = input_tokenizer.word_index\n",
    "input_idx2word = {idx: word for word, idx in input_word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create named entity to index and index to named entity mappings for the output sequence\n",
    "output_word2idx = output_tokenizer.word_index\n",
    "output_idx2word = {idx: word for word, idx in output_word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the input sequence\n",
    "max_input_len = max(len(seq) for seq in train_input_seq)\n",
    "train_input_seq = pad_sequences(train_input_seq, maxlen=max_input_len, padding='post')\n",
    "val_input_seq = pad_sequences(val_input_seq, maxlen=max_input_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_len\n",
    "# Pad the output sequence\n",
    "max_output_len = max(len(seq) for seq in train_output_seq)\n",
    "train_output_seq = pad_sequences(train_output_seq, maxlen=max_output_len, padding='post')\n",
    "val_output_seq = pad_sequences(val_output_seq, maxlen=max_output_len, padding='post')\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input and output dimensions for the model\n",
    "input_dim = len(input_word2idx) + 1\n",
    "output_dim = len(output_word2idx) + 1\n",
    "# Define the embedding dimension\n",
    "embedding_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of hidden units in the LSTM layer\n",
    "hidden_units = 256\n",
    "# import bidirectional LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "# import concatenate\n",
    "from tensorflow.keras.layers import Concatenate,Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input layer\n",
    "inputs = Input(shape=(max_input_len,))\n",
    "\n",
    "# Define the embedding layer\n",
    "embedding = Embedding(input_dim=input_dim, output_dim=embedding_dim)(inputs)\n",
    "\n",
    "# Define the Bidirectional LSTM layer\n",
    "lstm = Bidirectional(LSTM(hidden_units, return_sequences=True) )(embedding)\n",
    "\n",
    "# Define the output layer\n",
    "outputs = Dense(output_dim, activation='softmax')(lstm)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 5s 379ms/step - loss: 0.0498 - accuracy: 0.9860 - val_loss: 0.0452 - val_accuracy: 0.9859\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 3s 320ms/step - loss: 0.0428 - accuracy: 0.9864 - val_loss: 0.0390 - val_accuracy: 0.9870\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 3s 325ms/step - loss: 0.0371 - accuracy: 0.9877 - val_loss: 0.0366 - val_accuracy: 0.9871\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 2s 289ms/step - loss: 0.0352 - accuracy: 0.9883 - val_loss: 0.0353 - val_accuracy: 0.9875\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 2s 297ms/step - loss: 0.0341 - accuracy: 0.9883 - val_loss: 0.0344 - val_accuracy: 0.9886\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 2s 294ms/step - loss: 0.0331 - accuracy: 0.9890 - val_loss: 0.0334 - val_accuracy: 0.9886\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 2s 297ms/step - loss: 0.0320 - accuracy: 0.9890 - val_loss: 0.0322 - val_accuracy: 0.9887\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 2s 291ms/step - loss: 0.0304 - accuracy: 0.9894 - val_loss: 0.0303 - val_accuracy: 0.9889\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 2s 290ms/step - loss: 0.0280 - accuracy: 0.9903 - val_loss: 0.0272 - val_accuracy: 0.9920\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 2s 285ms/step - loss: 0.0244 - accuracy: 0.9925 - val_loss: 0.0231 - val_accuracy: 0.9936\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_input_seq, train_output_seq, validation_data=(val_input_seq, val_output_seq), batch_size=1024, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 87ms/step\n",
      "['B-LOC', 'B-LOC', 'STAT', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "طريق -> B-LOC\n",
      "حوارة -> B-LOC\n",
      "مسكر -> STAT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Use the model to make predictions on new input data\n",
    "text = \"طريق حوارة مسكر\"\n",
    "new_input_seq = input_tokenizer.texts_to_sequences([text])\n",
    "new_input_seq = pad_sequences(new_input_seq, maxlen=max_input_len, padding='post')\n",
    "\n",
    "pred_output_seq = model.predict(new_input_seq)\n",
    "pred_output_seq = np.argmax(pred_output_seq, axis=-1)\n",
    "\n",
    "pred_named_entities = [output_idx2word.get(idx, '') for idx in pred_output_seq[0]]\n",
    "print(pred_named_entities)\n",
    "# map each word to its predicted named entity\n",
    "for word, ner in zip(text.split(), pred_named_entities):\n",
    "    print(f\"{word} -> {ner}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finall",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
