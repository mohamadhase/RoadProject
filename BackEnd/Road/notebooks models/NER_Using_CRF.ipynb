{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "path = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "if path not in sys.path:\n",
    "    sys.path.append(path)\n",
    "from constants import NER\n",
    "# get the first key in NERa\n",
    "from fuzzywuzzy import fuzz\n",
    "from constants import NER\n",
    "from fuzzywuzzy import fuzz\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import sklearn_crfsuite\n",
    "import sklearn_crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../data/data_for_hajez_classifier.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import Parallel, delayed\n",
    "# import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_word(word, NER):\n",
    "#     for key in NER.keys():\n",
    "#         if any(fuzz.ratio(word, k) >= 80 for k in key):\n",
    "#             return NER[key]\n",
    "#     return \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_ner_string(text, NER):\n",
    "#     num_cores = multiprocessing.cpu_count()\n",
    "#     words = text.split()\n",
    "#     results = Parallel(n_jobs=num_cores)(\n",
    "#         delayed(process_word)(word, NER) for word in words)\n",
    "#     return \" \".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['ner'] = Parallel(n_jobs=-1)(\n",
    "#     delayed(generate_ner_string)(text, NER) for text in df['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get any row contains \"جيش\" in the text\n",
    "# df[df['full_text'].str.contains('اسرائيل')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('../data/data_with_ner.csv2', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/data_with_ner.csv2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop any row the full_text is \" \"\n",
    "df = df[df['full_text'] != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert full_text and ner to str\n",
    "df['full_text'] = df['full_text'].astype(str)\n",
    "df['ner'] = df['ner'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['full_text']\n",
    "y = df['ner']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarabic.araby as araby\n",
    "import pyarabic.araby as araby\n",
    "def is_punct(word):\n",
    "    \"\"\"\n",
    "    Returns True if the word is punctuation, False otherwise.\n",
    "    \"\"\"\n",
    "    # check if the word is punctuation\n",
    "    punct_chars = set('؟،؛?!.,:;')\n",
    "    return all(char in punct_chars for char in word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_it_bar(word):\n",
    "    \"\"\"\n",
    "    Returns True if the word is punctuation, False otherwise.\n",
    "    \"\"\"\n",
    "    from fuzzywuzzy import fuzz\n",
    "    barr_words = [\"دوار\",\"حاجز\",\"طريق\",\"مخصوم\",\"محسوم\"]\n",
    "    return any((fuzz.ratio(word, k) >= 80 for k in barr_words))\n",
    "    \n",
    "def is_it_status(word):\n",
    "    stat_words = [\"ازمه\",\"سهلة\",\"فاضيه\",\"سالك\",\"فاتحة\",\"ماشي\",\"ازمة\",\"فاتح\",\"مسكر\",\"فتح\",\"متوقف\",\"تفتيش\",\"مغلق\",\"سااالكه\",\n",
    "            \"سالك\",\"سالكة\",\"فاتح\",\"فاتحة\",\"فاضي\",\"مفتوح\",\"فتح\",\"خرجت\",\"ماشية\",\"ماشيه\",\"فش اشي\", \"سالك\",\"سألك\",\"نظيف\",\"نظيفة\",\n",
    "            \"فاضي\",\"نظيفه\",\"مسكر\",\"مغلق\",\"مغلقة\",\"اغلاق\",\"سكرو\",\"زاطمة\",\"زاطم\",\"سكر\",\"زطم\",\"واقف\",\"ازمة\", \"أزمة\", \"ازمه\", \"تشديد\",\n",
    "            \"تفتيش\"\"شرطه \",\"مستوطن\",\"شرطة\",\"جيش\",\"يهود\",\"حذر\"]\n",
    "    return any((fuzz.ratio(word, k) >= 80 for k in stat_words))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sub_words(output):\n",
    "    for index in range(len(output)):\n",
    "        if index < len(output) - 1:\n",
    "            if '##' in output[index+1]['word']:\n",
    "                output[index]['word'] = output[index]['word'] + output[index+1]['word']\n",
    "                output[index]['word'] = output[index]['word'].replace('##', '')\n",
    "                # delete the next word\n",
    "                output.pop(index+1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"CAMeL-Lab/bert-base-arabic-camelbert-ca-pos-egy\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define the pipeline\n",
    "nlp = pipeline(\"ner\", model=model_name, tokenizer=tokenizer)\n",
    "\n",
    "# # Encode a single input sequence\n",
    "# input_sequence = \"اليوم مريت على حاجز حوارة ولقيتو مفتوح\"\n",
    "# encoded_input = nlp(input_sequence)\n",
    "# encoded_input = combine_sub_words(encoded_input)\n",
    "# # Print each input token with the label\n",
    "# for word in encoded_input:\n",
    "#     print(word)\n",
    "#     # print(\"{} has label {}\".format(word[\"word\"], word[\"entity\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def word2features(sent,pos, i):\n",
    "    word = sent[i]\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word': word,\n",
    "        'word.ispunct()': is_punct(word),\n",
    "        'word.isnumeric()': word.isnumeric(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'word.isalpha()': word.isalpha(),\n",
    "        'word_len': len(word),\n",
    "        'word_prefix1': word[:1],\n",
    "        'word_prefix2': word[:2],\n",
    "        'word_prefix3': word[:3],\n",
    "        'word_prefix4': word[:4],\n",
    "        'word_suffix1': word[-1:],\n",
    "        'word_suffix2': word[-2:],\n",
    "        'word_suffix3': word[-3:],\n",
    "        'word_suffix4': word[-4:],\n",
    "        'word_pos': pos[i],\n",
    "    }\n",
    "    if i > 0:\n",
    "        prev_word = sent[i-1]\n",
    "        features.update({\n",
    "            'prev_word': prev_word,\n",
    "            'prev_word.ispunct()': is_punct(prev_word),\n",
    "            'prev_word.isnumeric()': prev_word.isnumeric(),\n",
    "            'prev_word.isdigit()': prev_word.isdigit(),\n",
    "            'prev_word.isalpha()': prev_word.isalpha(),\n",
    "            'prev_word_suffix1': prev_word[-1:],\n",
    "            'prev_word_suffix2': prev_word[-2:],\n",
    "            'prev_word_suffix3': prev_word[-3:],\n",
    "            'prev_word_suffix4': prev_word[-4:],\n",
    "            # 'prev_word_barr': is_it_bar(prev_word),\n",
    "            # 'prev_word_stat': is_it_status(prev_word),\n",
    "            'prev_word_pos': pos[i-1],\n",
    "        })\n",
    "    if i < len(sent)-1:\n",
    "        next_word = sent[i+1]\n",
    "        features.update({\n",
    "            'next_word': next_word,\n",
    "            'next_word.ispunct()': is_punct(next_word),\n",
    "            'next_word.isnumeric()': next_word.isnumeric(),\n",
    "            'next_word.isdigit()': next_word.isdigit(),\n",
    "            'next_word.isalpha()': next_word.isalpha(),\n",
    "            'next_word_prefix1': next_word[:1],\n",
    "            'next_word_prefix2': next_word[:2],\n",
    "            'next_word_prefix3': next_word[:3],\n",
    "            'next_word_prefix4': next_word[:4],\n",
    "            # 'next_word_barr': is_it_bar(next_word),\n",
    "            # 'next_word_stat': is_it_status(next_word),\n",
    "            'next_word_pos': pos[i+1]\n",
    "\n",
    "        })\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "    input = \" \".join(sent)\n",
    "    encoded_input = nlp(input)\n",
    "    encoded_input = combine_sub_words(encoded_input)\n",
    "    pos = [word[\"entity\"] for word in encoded_input]\n",
    "    \n",
    "    res = [word2features(sent,pos, i) for i in range(len(sent))]\n",
    "    if not res:\n",
    "        print(f\"res is empty {sent}\")\n",
    "    return res\n",
    "\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return list(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features = [sent2features(sent.split()) for sent in X_train]\n",
    "y_train_labels = [sent.split() for sent in y_train]\n",
    "\n",
    "X_test_features = [sent2features(sent.split()) for sent in X_test]\n",
    "y_test_labels = [sent.split() for sent in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bias': 1.0,\n",
       "  'word': 'مفرق',\n",
       "  'word.ispunct()': False,\n",
       "  'word.isnumeric()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'word.isalpha()': True,\n",
       "  'word_len': 4,\n",
       "  'word_prefix1': 'م',\n",
       "  'word_prefix2': 'مف',\n",
       "  'word_prefix3': 'مفر',\n",
       "  'word_prefix4': 'مفرق',\n",
       "  'word_suffix1': 'ق',\n",
       "  'word_suffix2': 'رق',\n",
       "  'word_suffix3': 'فرق',\n",
       "  'word_suffix4': 'مفرق',\n",
       "  'word_pos': 'adj',\n",
       "  'next_word': 'الطنيب',\n",
       "  'next_word.ispunct()': False,\n",
       "  'next_word.isnumeric()': False,\n",
       "  'next_word.isdigit()': False,\n",
       "  'next_word.isalpha()': True,\n",
       "  'next_word_prefix1': 'ا',\n",
       "  'next_word_prefix2': 'ال',\n",
       "  'next_word_prefix3': 'الط',\n",
       "  'next_word_prefix4': 'الطن',\n",
       "  'next_word_pos': 'noun'},\n",
       " {'bias': 1.0,\n",
       "  'word': 'الطنيب',\n",
       "  'word.ispunct()': False,\n",
       "  'word.isnumeric()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'word.isalpha()': True,\n",
       "  'word_len': 6,\n",
       "  'word_prefix1': 'ا',\n",
       "  'word_prefix2': 'ال',\n",
       "  'word_prefix3': 'الط',\n",
       "  'word_prefix4': 'الطن',\n",
       "  'word_suffix1': 'ب',\n",
       "  'word_suffix2': 'يب',\n",
       "  'word_suffix3': 'نيب',\n",
       "  'word_suffix4': 'طنيب',\n",
       "  'word_pos': 'noun',\n",
       "  'prev_word': 'مفرق',\n",
       "  'prev_word.ispunct()': False,\n",
       "  'prev_word.isnumeric()': False,\n",
       "  'prev_word.isdigit()': False,\n",
       "  'prev_word.isalpha()': True,\n",
       "  'prev_word_suffix1': 'ق',\n",
       "  'prev_word_suffix2': 'رق',\n",
       "  'prev_word_suffix3': 'فرق',\n",
       "  'prev_word_suffix4': 'مفرق',\n",
       "  'prev_word_pos': 'adj',\n",
       "  'next_word': 'سابك',\n",
       "  'next_word.ispunct()': False,\n",
       "  'next_word.isnumeric()': False,\n",
       "  'next_word.isdigit()': False,\n",
       "  'next_word.isalpha()': True,\n",
       "  'next_word_prefix1': 'س',\n",
       "  'next_word_prefix2': 'سا',\n",
       "  'next_word_prefix3': 'ساب',\n",
       "  'next_word_prefix4': 'سابك',\n",
       "  'next_word_pos': 'verb'},\n",
       " {'bias': 1.0,\n",
       "  'word': 'سابك',\n",
       "  'word.ispunct()': False,\n",
       "  'word.isnumeric()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'word.isalpha()': True,\n",
       "  'word_len': 4,\n",
       "  'word_prefix1': 'س',\n",
       "  'word_prefix2': 'سا',\n",
       "  'word_prefix3': 'ساب',\n",
       "  'word_prefix4': 'سابك',\n",
       "  'word_suffix1': 'ك',\n",
       "  'word_suffix2': 'بك',\n",
       "  'word_suffix3': 'ابك',\n",
       "  'word_suffix4': 'سابك',\n",
       "  'word_pos': 'verb',\n",
       "  'prev_word': 'الطنيب',\n",
       "  'prev_word.ispunct()': False,\n",
       "  'prev_word.isnumeric()': False,\n",
       "  'prev_word.isdigit()': False,\n",
       "  'prev_word.isalpha()': True,\n",
       "  'prev_word_suffix1': 'ب',\n",
       "  'prev_word_suffix2': 'يب',\n",
       "  'prev_word_suffix3': 'نيب',\n",
       "  'prev_word_suffix4': 'طنيب',\n",
       "  'prev_word_pos': 'noun'}]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_model = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any of X_train_features is empty\n",
    "indexes = []\n",
    "for i in range(len(X_train_features)):\n",
    "    if len(X_train_features[i]) == 0:\n",
    "        print(X_train_features[i]) # why is it empty?\n",
    "        \n",
    "# # get the corresponding rows in df \n",
    "# df.iloc[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    crf_model.fit(X_train_features, y_train_labels)\n",
    "except:\n",
    "    print(\"error\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy:  0.9999812120718701\n",
      "test accuracy:  0.998337816576156\n"
     ]
    }
   ],
   "source": [
    "# print the accuracy on the training set\n",
    "print(\"training accuracy: \", crf_model.score(X_train_features, y_train_labels))\n",
    "# print the accuracy on the test set\n",
    "print(\"test accuracy: \", crf_model.score(X_test_features, y_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = crf_model.predict(X_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.998337816576156\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Flatten the predictions and ground truth labels\n",
    "y_pred_flat = [tag for sent in y_pred for tag in sent]\n",
    "y_test_flat = [tag for sent in y_test_labels for tag in sent]\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test_flat, y_pred_flat)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Barr', 'B-LOC', 'O', 'STAT']"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text = \"طريق بيت شسيءؤئ مسكر\"\n",
    "new_text_features = sent2features(new_text.split())\n",
    "new_text_pred = crf_model.predict([new_text_features])[0]\n",
    "new_text_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "import joblib\n",
    "joblib.dump(crf_model, \"../models/NER_CRF.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict for df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finall",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
